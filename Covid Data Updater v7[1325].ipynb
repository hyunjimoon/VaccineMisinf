{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pandas numpy scipy statsmodels matplotlib gspread oauth2client\n",
    "\n",
    "Integrated data smoothing steps\n",
    "Fixed OWID-Wiki test data interaction - now takes exclusively OWID data from first available data, but will accept Wiki data before that\n",
    "\n",
    "**NOTE:** If getting errors during or at end of smoothing process, double check renames and droplist against updated OWID test data\n",
    "**NOTE 2:** If .csv files are showing up blank, check google sheet and make sure there are enough columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import gspread\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shutil import copy\n",
    "from scipy import interpolate\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sheets(sheet_key, testsheet_url, cred_json, scope, datalist, \n",
    "                    testsheet_renames=None, testsheet_droplist=None, retain_wikidata=None):\n",
    "    \"\"\" Downloads Google spreadsheet and saves each tab as CSV \"\"\"\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(cred_json, scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    book = gc.open_by_key(sheet_key)\n",
    "    print(\"Downloading data from Google sheets...\")\n",
    "    \n",
    "    def update_testdata(testdata, testsheet_url, testsheet_renames=None, \n",
    "                        testsheet_droplist=None, retain_wikidata=None):\n",
    "        \"\"\" Supplements testing numbers using data from OurWorldInData \n",
    "        and COVID tracking project (for US only) \"\"\"\n",
    "        # Download & read in test data from OurWorldInData spreadsheet\n",
    "        print(\"Downloading OWID test data...\")\n",
    "        testdf = pd.read_csv(testsheet_url)\n",
    "\n",
    "        # Set up OWID dataframe with index and column labels\n",
    "        testdf = testdf.filter(['Entity','Date','Cumulative total'], axis=1)\n",
    "        print(testdf)\n",
    "        testdf.Entity.astype(str)\n",
    "        \n",
    "        # Drop country-repeat data\n",
    "        if testsheet_droplist:\n",
    "            for d in testsheet_droplist:\n",
    "                testdf = testdf[testdf.Entity != d]    \n",
    "        \n",
    "        testdf.Entity = testdf.Entity.str.split(\" - \",expand=True)[0]\n",
    "        testdf.Date = ((pd.to_datetime(testdf.Date) - pd.datetime(*[2019,10,15])).dt.days * 1.0)\n",
    "        testdf = testdf[testdf.Date > 94]\n",
    "        testdf['Cumulative total'] = pd.to_numeric(testdf['Cumulative total'], errors='coerce')\n",
    "        \n",
    "        testdf = pd.pivot_table(testdf, index='Entity', columns='Date')\n",
    "        testdf.drop(['United States'], inplace=True) # Drop USA data (will get from covidtracking.com)\n",
    "        if testsheet_renames:\n",
    "            testdf.rename(index=testsheet_renames, inplace=True)    \n",
    "\n",
    "        # Set up testdata dataframe with index and column labels\n",
    "        header = testdata.iloc[0]\n",
    "        header[2:] = header[2:].astype(int)\n",
    "        testdata = testdata[1:]\n",
    "        testdata.columns = header        \n",
    "        testdata.index = testdata.iloc[:,1]\n",
    "        print(testdf)\n",
    "\n",
    "        # Write any OWID data over testdata values starting from first available date of OWID data\n",
    "        print(\"Updating OWID test data...\")\n",
    "        for rownum, row in testdf.iterrows():\n",
    "            if rownum in retain_wikidata:\n",
    "                row.loc[:get_first_idx(row)] = -1 # Identify pre-first date data with placeholder\n",
    "                for idx, val in row.items():\n",
    "                    if testdf.at[rownum, idx] != -1:\n",
    "                        testdata.at[rownum, idx[1]] = val\n",
    "            else:\n",
    "                for idx, val in row.items():\n",
    "                    testdata.at[rownum, idx[1]] = val\n",
    "\n",
    "        # Download US test data from covidtracking.com\n",
    "        print(\"Updating US test data...\")\n",
    "        usdaily_json = requests.get('https://covidtracking.com/api/us/daily').json()\n",
    "        usdaily = pd.DataFrame(usdaily_json)\n",
    "        ustotal = usdaily.filter(['date','totalTestResults'], axis=1)\n",
    "\n",
    "        ustotal.date = ((pd.to_datetime(ustotal.date, format='%Y%m%d') - pd.datetime(*[2019,10,15])).dt.days * 1.0)\n",
    "\n",
    "        ustotal.index = ustotal.date\n",
    "        usdata = ustotal.drop('date', axis=1).T\n",
    "        usdata.rename(index={'totalTestResults':'USA'}, inplace=True)\n",
    "\n",
    "        for col, series in usdata.items():\n",
    "            for row, val in series.items():\n",
    "                testdata.at[row, col] = val\n",
    "        \n",
    "        limit = (pd.to_datetime(\"today\") - pd.datetime(*[2019,10,15])).days - 95\n",
    "        testdata = testdata.columns.to_frame().T.append(testdata, ignore_index=True)\n",
    "        testdata = testdata.iloc[:, :limit]\n",
    "        return testdata\n",
    "    \n",
    "    for dataname in datalist:\n",
    "        worksheet = book.worksheet(dataname)\n",
    "        table = worksheet.get_all_values()\n",
    "        dataframe = pd.DataFrame(table)\n",
    "        \n",
    "        # For column 2 in ConstantData, strip % signs and convert to float\n",
    "        if dataname == \"ConstantData\":\n",
    "            dataframe[3][1:] = pd.to_numeric(dataframe[3][1:].str.strip('%')).div(100)\n",
    "        \n",
    "        # For row 191 (USA) in TestData, replce with CovidTracker data\n",
    "        if dataname == \"TestData\":\n",
    "            dataframe = update_testdata(dataframe, testsheet_url, testsheet_renames, \n",
    "                                        testsheet_droplist, retain_wikidata)\n",
    "            \n",
    "        dataframe.to_csv(f'{dataname}.csv', header=False, index=False)\n",
    "\n",
    "        \n",
    "def import_datasets(datalist, vdfname):\n",
    "    \"\"\" Creates Vensim script to convert CSVs to VDFXs \"\"\"\n",
    "    print(\"Importing data to VDF...\")\n",
    "    scenario_text = []\n",
    "    scenario_text.append(\"SPECIAL>NOINTERACTION\\n\")\n",
    "    \n",
    "    for dataname in datalist:\n",
    "        scenario_text.append(f\"MENU>CSV2VDF|{dataname}.csv|{vdfname}{dataname}.vdfx|{dataname}.frm|\\n\")\n",
    "    \n",
    "    scenario_text.append(\"MENU>EXIT\\n\")\n",
    "    \n",
    "    scriptfile = open(\"ImportData.cmd\", 'w')\n",
    "    scriptfile.writelines(scenario_text)\n",
    "    scriptfile.close()\n",
    "\n",
    "    \n",
    "def copy_data(datalist, vdfname):\n",
    "    \"\"\" Copies VDFXs to parent directory of working directory \"\"\"\n",
    "    for dataname in datalist:\n",
    "        for filetype in [\".vdf\", \".vdfx\"]:\n",
    "            try:\n",
    "                copy(f\"./{vdfname}{dataname}{filetype}\", f\"../\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "            \n",
    "def idx_to_int(df):\n",
    "    \"\"\"Converts string numeric column keys of dataframe to int\"\"\"\n",
    "    Tdf = df.T\n",
    "    Tdf.index = Tdf.index.astype('float').astype('int')\n",
    "    newdf = Tdf.T\n",
    "    return(newdf)\n",
    "\n",
    "\n",
    "def get_first_idx(s):\n",
    "    return (s > 0).idxmax(skipna=True)\n",
    "\n",
    "\n",
    "def get_last_idx(s):\n",
    "    return s.where(s > 0).last_valid_index()\n",
    "\n",
    "\n",
    "def calculate_devs(flowrow, windowlength, datathreshold, thresholdwidth=1):\n",
    "    \"\"\"Calculate rolling mean of series and adjusted deviations from the mean, as well as \n",
    "    threshold values based on median +/- MADs, ignoring values below given datathreshold\"\"\"\n",
    "    flowmeanraw = flowrow.rolling(windowlength, min_periods=1, center=True).mean()\n",
    "    flowmean = flowmeanraw.copy()\n",
    "    flowmean.loc[:(flowmean >= datathreshold).idxmax()] = np.nan\n",
    "    flowrawdev = flowrow - flowmean\n",
    "    flowadjdev = flowrawdev / np.sqrt(flowmean)\n",
    "    lowthreshold = flowadjdev.median() - flowadjdev.mad() * thresholdwidth\n",
    "    highthreshold = flowadjdev.median() + flowadjdev.mad() * thresholdwidth\n",
    "    devs = {'rawmean': flowmeanraw, 'mean': flowmean, 'rawdev': flowrawdev, \n",
    "            'adjdev': flowadjdev, 'lowthr': lowthreshold, 'highthr': highthreshold}\n",
    "    return devs\n",
    "\n",
    "\n",
    "def fill_dips(smflow, smdevs, k, smoothfactor, lowthreshold, borrowlength=7):\n",
    "    \"\"\"Identify points with deviations below threshold value and partially fill \n",
    "    by borrowing from following points, based on a multinomial draw with probabilities \n",
    "    proportional to deviations of those points\"\"\"\n",
    "    for i, adjdev in enumerate(smdevs['adjdev'][:-k]):\n",
    "        if adjdev < lowthreshold:\n",
    "            borrowlist = smdevs['adjdev'].iloc[i+1:max(i+1+borrowlength, i+1)]\n",
    "            values = smflow.iloc[i+1:max(i+1+borrowlength, i+1)]\n",
    "            borrowlist -= adjdev\n",
    "            borrowlist.mask(borrowlist < 0, other=0, inplace=True)\n",
    "            if not all([(b == 0 or np.isnan(b)) for b in borrowlist]):\n",
    "                borrowlist.astype('float64')\n",
    "                borrowlist.dropna(inplace=True)\n",
    "                borrowlist /= borrowlist.sum()\n",
    "                mnlist = np.random.multinomial(abs(int(np.floor(smdevs['rawdev'].iloc[i]*smoothfactor))), \n",
    "                                               [abs(i) for i in borrowlist])\n",
    "                minlen = min(len(mnlist), len(values))\n",
    "                mnlist = np.minimum(mnlist[:minlen], values[:minlen])\n",
    "                smflow.iloc[i] += mnlist.sum()\n",
    "                for j, val in enumerate(mnlist):\n",
    "                    smflow.iloc[i+1+j] -= val\n",
    "\n",
    "                \n",
    "def smooth_peaks(smflow, smdevs, k, smoothfactor, highthreshold, distlength=14):\n",
    "    \"\"\"Identify points with deviations above threshold value and partially flatten \n",
    "    by distributing to preceding points, based on a multinomial draw with probabilities \n",
    "    proportional to existing rolling means of those points\"\"\"\n",
    "    for i, adjdev in reversed(list(enumerate(smdevs['adjdev'][:-k]))):\n",
    "        if adjdev > highthreshold:\n",
    "            distlist = smdevs['rawmean'].iloc[max(0, i-distlength):i]\n",
    "            if not all([(d == 0 or np.isnan(d)) for d in distlist]):\n",
    "                distlist.astype('float64')\n",
    "                distlist /= distlist.sum()\n",
    "                mnlist = np.random.multinomial(abs(int(np.floor(smdevs['rawdev'].iloc[i]*smoothfactor))), distlist)\n",
    "                smflow.iloc[i] -= mnlist.sum()\n",
    "                for j, val in enumerate(mnlist):\n",
    "                    smflow.iloc[i-len(mnlist)+j] += val\n",
    "\n",
    "\n",
    "def iter_smooth(smflow, ordevs, windowlength, datathreshold, smoothfactor, \n",
    "                borrowlength=7, distlength=14, iterlimit=10):\n",
    "    \"\"\"Iteratively apply dip-filling and peak-smoothing algorithms until \n",
    "    all deviations are within the upper and lower median+/-MAD thresholds\"\"\"\n",
    "    smdevs = calculate_devs(smflow, windowlength, datathreshold)\n",
    "    i = 0\n",
    "    while i < iterlimit:\n",
    "        # If mean values are too low, skip all smoothing\n",
    "        if np.nanmax(smdevs['mean']) < datathreshold:\n",
    "            break\n",
    "        # Identify last valid index and check if below threshold\n",
    "        k = smflow.index.get_loc(get_last_idx(smflow))\n",
    "        k = len(smflow) - k\n",
    "        # Identify all consecutive final terms below threshold to skip, otherwise will cause errors\n",
    "        while smdevs['adjdev'].iloc[-k] < ordevs['lowthr']:\n",
    "            k +=1\n",
    "        if np.nanmin(smdevs['adjdev'][:-k]) < ordevs['lowthr']:\n",
    "            fill_dips(smflow, smdevs, k, smoothfactor, ordevs['lowthr'])\n",
    "            smdevs = calculate_devs(smflow, windowlength, datathreshold)\n",
    "        if np.nanmax(smdevs['adjdev'][:-k]) > ordevs['highthr']:\n",
    "            smooth_peaks(smflow, smdevs, k, smoothfactor, ordevs['highthr'])\n",
    "            smdevs = calculate_devs(smflow, windowlength, datathreshold)\n",
    "        if (np.nanmax(smdevs['adjdev'][:-k]) < ordevs['highthr'] \n",
    "            and np.nanmin(smdevs['adjdev'][:-k]) > ordevs['lowthr']):\n",
    "            break\n",
    "        i += 1\n",
    "    return smflow\n",
    "\n",
    "\n",
    "def cross_corr(x, y, shift):\n",
    "    \"\"\"Get time-shifted cross-correlations of two series\"\"\"\n",
    "    if shift > 0:\n",
    "        xshift = x[0:-shift]\n",
    "        yshift = y[shift:]\n",
    "    elif shift < 0:\n",
    "        xshift = x[-shift:]\n",
    "        yshift = y[0:shift]\n",
    "    elif shift == 0:\n",
    "        xshift = x\n",
    "        yshift = y\n",
    "\n",
    "    rawcorrs = np.correlate(xshift, yshift, mode='full')\n",
    "    normcorr = rawcorrs[(rawcorrs.size // 2):] / np.amax(rawcorrs)\n",
    "    \n",
    "    return normcorr[0]\n",
    "\n",
    "\n",
    "def time_shift(x, shift):\n",
    "    \"\"\"Shift a series by a specified amount\"\"\"\n",
    "    xshift = x.copy()\n",
    "    if shift > 0:\n",
    "        xshift[shift:] = x[0:-shift]\n",
    "    elif shift < 0:\n",
    "        xshift[0:shift] = x[-shift:]\n",
    "    elif shift == 0:\n",
    "        pass\n",
    "    return xshift\n",
    "\n",
    "    \n",
    "def smooth_data(skiplist):\n",
    "    \"\"\"Run data smoothing and time shifting on data\"\"\"    \n",
    "    print(\"Executing smoothing algorithm!\")\n",
    "        \n",
    "    # Import dataframes from CSV and drop variable names\n",
    "    testdf = pd.read_csv(\"TestData.csv\", index_col=1,header=0)\n",
    "    testdf.drop(columns='Time', inplace=True)\n",
    "\n",
    "    formdf = pd.read_csv(\"FormattedData.csv\", index_col=1,header=0)\n",
    "    formdf.drop(columns='Time', inplace=True)\n",
    "\n",
    "    flowdf = pd.read_csv(\"FlowData.csv\",index_col=1,header=0)\n",
    "    flowdf.drop(columns='Time', inplace=True)\n",
    "\n",
    "    # Convert string indices to int\n",
    "    testdf = idx_to_int(testdf)\n",
    "    formdf = idx_to_int(formdf)\n",
    "    flowdf = idx_to_int(flowdf)\n",
    "    \n",
    "    display(testdf)\n",
    "\n",
    "    # Set up sub-dataframes from main data files\n",
    "    infdf = flowdf[0:nrows].copy()\n",
    "    dthdf = flowdf[nrows:(nrows*2)].copy()\n",
    "    recdf = flowdf[(nrows*2):(nrows*3)].copy()\n",
    "#     tratedf = pd.DataFrame(data=None, columns=testdf.columns, index=testdf.index)\n",
    "#     tcapdf = pd.DataFrame(data=None, columns=testdf.columns, index=testdf.index)\n",
    "#     tratedf = testdf.replace(testdf, np.nan)\n",
    "#     tcapdf = testdf.replace(testdf, np.nan)\n",
    "    tratedf = pd.DataFrame().reindex_like(testdf)\n",
    "    tcapdf = pd.DataFrame().reindex_like(testdf)\n",
    "    \n",
    "    # Convert infinite values to NaN to avoid potential errors\n",
    "    testdf.replace([np.inf, -np.inf], np.NaN)\n",
    "    \n",
    "    for i in testdf.index:\n",
    "        # Check if country is in skiplist\n",
    "        if i in skiplist:\n",
    "            print(f\"Repressing {i}!\")\n",
    "            continue\n",
    "        \n",
    "        # Check if country has sufficient test data to proceed, else skip\n",
    "        elif len(testdf.loc[i].dropna()) > mintestpoints:\n",
    "\n",
    "            # Ensure cumulative test data is strictly monotonic increasing\n",
    "            # NOTE: if monotonicity check happens after date value assignment, \n",
    "            # then if last test data point is nonmonotonic, it will be dropped causing an error\n",
    "            testdf.loc[i] = testdf.loc[i].mask(testdf.loc[i].cummax().duplicated())\n",
    "\n",
    "            # Identify first and last infection, test, and death date indices\n",
    "            infA, testA = [get_first_idx(s) for s in [infdf.loc[i], testdf.loc[i]]]\n",
    "            infZ, testZ, dthZ = [get_last_idx(s) for s in [infdf.loc[i], testdf.loc[i], dthdf.loc[i]]]\n",
    "\n",
    "            # Assign 0 test value to first infection date if before first test date\n",
    "            if infA < testA:\n",
    "                newtestA = infA\n",
    "                testdf.loc[i, newtestA] = 0\n",
    "            else:\n",
    "                newtestA = testA\n",
    "\n",
    "            # Set test rate and capacity values to 0 before first data point\n",
    "            tratedf.loc[i, :newtestA], tcapdf.loc[i, :newtestA] = 0, 0\n",
    "\n",
    "            # Check whether original test data is sparse in latter half of test data window\n",
    "            halftestrow = testdf.loc[i, newtestA:testZ]\n",
    "            halftestrow = halftestrow.iloc[len(halftestrow)//2:]\n",
    "            if len(halftestrow.dropna())/len(halftestrow) > 0.5:\n",
    "                smcheck = False\n",
    "            else:\n",
    "                smcheck = True\n",
    "                print(i, \"is sparse:\", len(testdf.loc[i]), len(halftestrow), len(halftestrow.dropna()))\n",
    "\n",
    "            # Interpolate test data using PCHIP spline if possible, within range of presumed test data\n",
    "            spline = interpolate.PchipInterpolator(testdf.loc[i].dropna().index, testdf.loc[i].dropna().values)\n",
    "            interptests = spline(testdf.loc[i, newtestA:testZ].index)\n",
    "\n",
    "            # Check if any interpolated values are negative; if so do linear interpolation instead\n",
    "            if any((interptests[1:] - interptests[:-1]) < 0):\n",
    "                print(\"Uh-oh, negative spline result, going linear!\")\n",
    "                linear = interpolate.interp1d(testdf.loc[i].dropna().index, testdf.loc[i].dropna().values)\n",
    "                interptests = linear(testdf.loc[i, newtestA:testZ].index)\n",
    "\n",
    "            # Assign interpolated values back to test data\n",
    "            testdf.loc[i, newtestA:testZ] = interptests\n",
    "            tratedf.loc[i, newtestA:testZ] = np.insert((interptests[1:] - interptests[:-1]), 0, interptests[0])\n",
    "\n",
    "            # If original test data is sparse, smooth test and infection data\n",
    "            if smcheck:\n",
    "                tratedevs = calculate_devs(tratedf.loc[i, newtestA:testZ], windowlength, datathreshold)\n",
    "                tratedf.loc[i, newtestA:testZ] = iter_smooth(tratedf.loc[i, newtestA:testZ], tratedevs, \n",
    "                                                             windowlength, datathreshold, smoothfactor)\n",
    "                infdevs = calculate_devs(infdf.loc[i, :infZ], windowlength, datathreshold)\n",
    "                infdf.loc[i, :infZ] = iter_smooth(infdf.loc[i, :infZ], infdevs, windowlength, datathreshold, smoothfactor)\n",
    "\n",
    "            # Else if original test data not sparse, do time shift on test data\n",
    "            else:\n",
    "                minlen = min(len(tratedf.loc[i].dropna()), len(infdf.loc[i].dropna()))\n",
    "                if minlen == 0:\n",
    "                    print(f\"Insufficient data for {i} shift, skipping!\")\n",
    "                else:\n",
    "                    x = STL(tratedf.loc[i].dropna(), period=7, seasonal=7).fit().seasonal\n",
    "                    y = STL(infdf.loc[i].dropna(), period=7, seasonal=7).fit().seasonal\n",
    "\n",
    "                    alseas = x.align(y, join='inner')\n",
    "\n",
    "                    seascorrs = []\n",
    "                    shiftrange = list(range(-2,5))\n",
    "\n",
    "                    for j in shiftrange:\n",
    "                        seascorrs.append(cross_corr(alseas[0], alseas[1], j))\n",
    "\n",
    "                    tshift = shiftrange[np.argmax(seascorrs)]\n",
    "                    shifttrate = time_shift(tratedf.loc[i], tshift)\n",
    "\n",
    "                    tratedf.loc[i] = shifttrate\n",
    "                    newtestA += tshift\n",
    "                    testZ += tshift\n",
    "\n",
    "                    print(f\"{i} shift is {tshift}\")\n",
    "\n",
    "            # Run polyfit on test rate data for later use to estimate test capacity\n",
    "            # Test capacity will be estimated as max of fitted test rate LATER on whole DF\n",
    "            pfit = np.polyfit(tratedf.loc[i, newtestA:testZ].index, tratedf.loc[i, newtestA:testZ].values, 10)\n",
    "            tcapdf.loc[i, newtestA:testZ] = np.polyval(pfit, tratedf.loc[i, newtestA:testZ].index)\n",
    "\n",
    "            # Run iterative dip/peak smoothing on death rates for all countries with enough deaths\n",
    "            if np.nanmax(dthdf.loc[i]) > datathreshold:\n",
    "                dthdevs = calculate_devs(dthdf.loc[i, :dthZ], windowlength, datathreshold)\n",
    "                dthdf.loc[i, :dthZ] = iter_smooth(dthdf.loc[i, :dthZ], dthdevs, windowlength, datathreshold, smoothfactor)\n",
    "                \n",
    "        else:\n",
    "            print(f\"Not enough test data for {i}, skipping!\")\n",
    "    \n",
    "    # Combine flow data streams into one dataframe\n",
    "    smflowdf = pd.concat([infdf, dthdf, recdf], axis=0)\n",
    "\n",
    "    # Set test capacity based on polyfit of test rate, ignoring first day\n",
    "    tcapdf.iloc[:, 1:] = tcapdf.iloc[:, 1:].cummax(axis=1, skipna=False)\n",
    "\n",
    "    # Recalculate cumulative tests based on smoothed test data\n",
    "    testdf = tratedf.cumsum(axis=1, skipna=False)\n",
    "\n",
    "    # Combine all three test data streams into one dataframe, dropping first day\n",
    "    smtestdf = pd.concat([testdf, tratedf, tcapdf], axis=0).iloc[:,1:]\n",
    "\n",
    "    # Shave NANs and last column of test dataframe\n",
    "    smtestdf.dropna(axis=1, how='all', inplace=True)\n",
    "    smtestdf = smtestdf.iloc[:,:-1]\n",
    "\n",
    "    # Adjust first day flows to account for non-zero initial cumulative values\n",
    "    smflowdf.iloc[:,0] += formdf.iloc[:,0]\n",
    "\n",
    "    # Recalculate cumulative data from smoothed flows, then readjust first day flows\n",
    "    smformdf = smflowdf.cumsum(axis=1)\n",
    "    smflowdf.iloc[:,0] -= formdf.iloc[:,0]\n",
    "\n",
    "    # Restore variable names and export to CSV\n",
    "    smflowdf.reset_index(inplace=True)\n",
    "    smflowdf.insert(0, 'Time', ['DataFlowInfection']*nrows+['DataFlowDeath']*nrows+['DataFlowRecovery']*nrows)\n",
    "    smflowdf.to_csv(\"FlowData.csv\", index=False)\n",
    "\n",
    "    smtestdf.reset_index(inplace=True)\n",
    "    smtestdf.insert(0, 'Time', ['DataCmltTest']*nrows+['DataTestRate']*nrows+['DataTestCapacity']*nrows)\n",
    "    smtestdf.to_csv(\"TestData.csv\", index=False)\n",
    "\n",
    "    smformdf.reset_index(inplace=True)\n",
    "    smformdf.insert(0, 'Time', ['DataCmltInfection']*nrows+['DataCmltDeath']*nrows+['DataCmltRecovery']*nrows)\n",
    "    smformdf.to_csv(\"FormattedData.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter control file name (with extension):V7DUControl.txt\n",
      "Downloading data from Google sheets...\n",
      "Downloading OWID test data...\n",
      "                           Entity        Date  Cumulative total\n",
      "0       Argentina - people tested  2020-01-01               1.0\n",
      "1       Argentina - people tested  2020-02-03               2.0\n",
      "2       Argentina - people tested  2020-02-04               3.0\n",
      "3       Argentina - people tested  2020-02-05               NaN\n",
      "4       Argentina - people tested  2020-02-06               NaN\n",
      "...                           ...         ...               ...\n",
      "31620  Zimbabwe - tests performed  2020-12-17          193011.0\n",
      "31621  Zimbabwe - tests performed  2020-12-18          195166.0\n",
      "31622  Zimbabwe - tests performed  2020-12-19          196747.0\n",
      "31623  Zimbabwe - tests performed  2020-12-20          198891.0\n",
      "31624  Zimbabwe - tests performed  2020-12-21          200096.0\n",
      "\n",
      "[31625 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Cumulative total                                                  \\\n",
      "Date                  95.0  96.0  97.0  98.0  99.0  100.0 101.0 102.0 103.0   \n",
      "Entity                                                                        \n",
      "Argentina               NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Australia               NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Austria                 NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Bahrain                 NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Bangladesh              NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "...                     ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "UK                      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Uruguay                 NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Vietnam                 NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Zambia                  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "Zimbabwe                NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "                  ...                                                  \\\n",
      "Date       104.0  ...       426.0       427.0       428.0       429.0   \n",
      "Entity            ...                                                   \n",
      "Argentina    NaN  ...   3643669.0   3666548.0   3689675.0   3714050.0   \n",
      "Australia    NaN  ...         NaN  10419286.0  10451140.0  10479855.0   \n",
      "Austria      NaN  ...   3425072.0   3446109.0   3474856.0   3503072.0   \n",
      "Bahrain      NaN  ...   2191887.0         NaN   2214897.0   2224692.0   \n",
      "Bangladesh   NaN  ...   2984285.0   3003339.0   3020364.0   3033555.0   \n",
      "...          ...  ...         ...         ...         ...         ...   \n",
      "UK           NaN  ...  45696137.0  46009318.0  46373777.0  46771368.0   \n",
      "Uruguay      NaN  ...         NaN    521173.0    528583.0    537278.0   \n",
      "Vietnam      NaN  ...         NaN         NaN         NaN         NaN   \n",
      "Zambia       NaN  ...    489926.0    493685.0    498639.0    505896.0   \n",
      "Zimbabwe     NaN  ...    189141.0    189700.0    191052.0    193011.0   \n",
      "\n",
      "                                                                        \\\n",
      "Date             430.0       431.0       432.0       433.0       434.0   \n",
      "Entity                                                                   \n",
      "Argentina          NaN         NaN         NaN         NaN         NaN   \n",
      "Australia          NaN         NaN  10579410.0  10649443.0  10725537.0   \n",
      "Austria      3534486.0   3568286.0   3593276.0   3614233.0         NaN   \n",
      "Bahrain      2234549.0   2243353.0   2252164.0   2262042.0   2272640.0   \n",
      "Bangladesh   3047891.0   3060191.0   3072491.0   3088160.0   3103305.0   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "UK          47162512.0  47603238.0  48036807.0  48469931.0         NaN   \n",
      "Uruguay       544455.0    552704.0    559257.0    565207.0    572581.0   \n",
      "Vietnam            NaN   1469955.0         NaN         NaN         NaN   \n",
      "Zambia        513928.0    518986.0    524876.0    531000.0    536507.0   \n",
      "Zimbabwe      195166.0    196747.0    198891.0    200096.0         NaN   \n",
      "\n",
      "                       \n",
      "Date            435.0  \n",
      "Entity                 \n",
      "Argentina         NaN  \n",
      "Australia         NaN  \n",
      "Austria           NaN  \n",
      "Bahrain     2282216.0  \n",
      "Bangladesh        NaN  \n",
      "...               ...  \n",
      "UK                NaN  \n",
      "Uruguay           NaN  \n",
      "Vietnam           NaN  \n",
      "Zambia            NaN  \n",
      "Zimbabwe          NaN  \n",
      "\n",
      "[105 rows x 341 columns]\n",
      "Updating OWID test data...\n",
      "Updating US test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:73: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n"
     ]
    }
   ],
   "source": [
    "controlfilename = input(\"Enter control file name (with extension):\")\n",
    "controlfile = json.load(open(controlfilename, 'r'))\n",
    "\n",
    "# Unpack controlfile into variables\n",
    "for k,v in controlfile.items():\n",
    "    exec(k + '=v')\n",
    "\n",
    "download_sheets(sheet_key, testsheet_url, cred_json, scope, datalist,\n",
    "                testsheet_renames, testsheet_droplist, retain_wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing smoothing algorithm!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>...</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andorra</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venezuela</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1469955.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yemen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zambia</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>471542.0</td>\n",
       "      <td>477174.0</td>\n",
       "      <td>484299.0</td>\n",
       "      <td>489926.0</td>\n",
       "      <td>493685.0</td>\n",
       "      <td>498639.0</td>\n",
       "      <td>505896.0</td>\n",
       "      <td>513928.0</td>\n",
       "      <td>518986.0</td>\n",
       "      <td>524876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zimbabwe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>184964.0</td>\n",
       "      <td>186328.0</td>\n",
       "      <td>187515.0</td>\n",
       "      <td>189141.0</td>\n",
       "      <td>189700.0</td>\n",
       "      <td>191052.0</td>\n",
       "      <td>193011.0</td>\n",
       "      <td>195166.0</td>\n",
       "      <td>196747.0</td>\n",
       "      <td>198891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 338 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             95   96   97   98   99   100  101  102  103  104  ...       423  \\\n",
       "Afghanistan  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Albania      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Algeria      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Andorra      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Angola       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "...          ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "Venezuela    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Vietnam      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Yemen        NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...       NaN   \n",
       "Zambia       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  471542.0   \n",
       "Zimbabwe     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  184964.0   \n",
       "\n",
       "                  424       425       426       427       428       429  \\\n",
       "Afghanistan       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Albania           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Algeria           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Andorra           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Angola            NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "Venezuela         NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Vietnam           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Yemen             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Zambia       477174.0  484299.0  489926.0  493685.0  498639.0  505896.0   \n",
       "Zimbabwe     186328.0  187515.0  189141.0  189700.0  191052.0  193011.0   \n",
       "\n",
       "                  430        431       432  \n",
       "Afghanistan       NaN        NaN       NaN  \n",
       "Albania           NaN        NaN       NaN  \n",
       "Algeria           NaN        NaN       NaN  \n",
       "Andorra           NaN        NaN       NaN  \n",
       "Angola            NaN        NaN       NaN  \n",
       "...               ...        ...       ...  \n",
       "Venezuela         NaN        NaN       NaN  \n",
       "Vietnam           NaN  1469955.0       NaN  \n",
       "Yemen             NaN        NaN       NaN  \n",
       "Zambia       513928.0   518986.0  524876.0  \n",
       "Zimbabwe     195166.0   196747.0  198891.0  \n",
       "\n",
       "[199 rows x 338 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough test data for Afghanistan, skipping!\n",
      "Albania is sparse: 338 47 9\n",
      "Not enough test data for Algeria, skipping!\n",
      "Not enough test data for Andorra, skipping!\n",
      "Not enough test data for Angola, skipping!\n",
      "Not enough test data for Antigua, skipping!\n",
      "Argentina shift is 0\n",
      "Armenia is sparse: 338 43 7\n",
      "Australia shift is -1\n",
      "Austria shift is 0\n",
      "Azerbaijan is sparse: 338 46 8\n",
      "Not enough test data for Bahamas, skipping!\n",
      "Bahrain shift is 2\n",
      "Bangladesh shift is 0\n",
      "Not enough test data for Barbados, skipping!\n",
      "Belarus is sparse: 338 135 47\n",
      "Belgium shift is 1\n",
      "Not enough test data for Belize, skipping!\n",
      "Not enough test data for Benin, skipping!\n",
      "Not enough test data for Bhutan, skipping!\n",
      "Bolivia shift is 0\n",
      "Bosnia shift is -1\n",
      "Not enough test data for Botswana, skipping!\n",
      "Repressing Brazil!\n",
      "Not enough test data for Brunei, skipping!\n",
      "Bulgaria shift is -1\n",
      "Not enough test data for Burkina Faso, skipping!\n",
      "Not enough test data for Burundi, skipping!\n",
      "Not enough test data for CaboVerde, skipping!\n",
      "Not enough test data for Cambodia, skipping!\n",
      "Not enough test data for Cameroon, skipping!\n",
      "Canada shift is 3\n",
      "Not enough test data for CAR, skipping!\n",
      "Not enough test data for Chad, skipping!\n",
      "Chile shift is 0\n",
      "Not enough test data for China, skipping!\n",
      "Colombia shift is 0\n",
      "Not enough test data for Comoros, skipping!\n",
      "Not enough test data for CongoRepublic, skipping!\n",
      "CostaRica shift is 0\n",
      "Croatia shift is 0\n",
      "Cuba shift is -1\n",
      "Cyprus shift is 0\n",
      "CzechRepublic shift is 0\n",
      "Denmark shift is 2\n",
      "Not enough test data for DiamonPrinces, skipping!\n",
      "Not enough test data for Djibouti, skipping!\n",
      "Not enough test data for Dominica, skipping!\n",
      "DominicanRepublic shift is 1\n",
      "Not enough test data for DRC, skipping!\n",
      "Not enough test data for EastTimor, skipping!\n",
      "Ecuador shift is 0\n",
      "Not enough test data for Egypt, skipping!\n",
      "ElSalvador shift is 0\n",
      "Not enough test data for EquatorialGuinea, skipping!\n",
      "Not enough test data for Eritrea, skipping!\n",
      "Estonia shift is 1\n",
      "Not enough test data for Eswatini, skipping!\n",
      "Ethiopia shift is 0\n",
      "Fiji shift is -1\n",
      "Finland shift is 1\n",
      "France is sparse: 338 66 10\n",
      "Not enough test data for Gabon, skipping!\n",
      "Not enough test data for Gambia, skipping!\n",
      "Not enough test data for Georgia, skipping!\n",
      "Germany is sparse: 338 161 23\n",
      "Ghana shift is -1\n",
      "Greece shift is 0\n",
      "Not enough test data for Grenada, skipping!\n",
      "Not enough test data for Grenadines, skipping!\n",
      "Guatemala shift is 1\n",
      "Not enough test data for Guinea, skipping!\n",
      "Not enough test data for GuineaBissau, skipping!\n",
      "Not enough test data for Guyana, skipping!\n",
      "Not enough test data for Haiti, skipping!\n",
      "Not enough test data for Honduras, skipping!\n",
      "HongKong is sparse: 338 164 8\n",
      "Hungary shift is 0\n",
      "Iceland shift is 1\n",
      "India shift is 0\n",
      "Indonesia shift is 1\n",
      "Iran shift is 0\n",
      "Iraq shift is 0\n",
      "Ireland shift is 1\n",
      "Israel shift is 0\n",
      "Italy shift is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:233: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IvoryCoast shift is -2\n",
      "Jamaica shift is 0\n",
      "Japan shift is 1\n",
      "Jordan is sparse: 338 147 50\n",
      "Kazakhstan shift is -2\n",
      "Kenya shift is 0\n",
      "Not enough test data for Kiribati, skipping!\n",
      "Not enough test data for Kosovo, skipping!\n",
      "Kuwait shift is 0\n",
      "Kyrgyzstan is sparse: 338 25 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RankWarning: Polyfit may be poorly conditioned\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough test data for Laos, skipping!\n",
      "Latvia shift is 0\n",
      "Not enough test data for Lebanon, skipping!\n",
      "Not enough test data for Lesotho, skipping!\n",
      "Not enough test data for Liberia, skipping!\n",
      "Not enough test data for Libya, skipping!\n",
      "Not enough test data for Liechtenstein, skipping!\n",
      "Lithuania shift is 2\n",
      "Luxembourg shift is -1\n",
      "Not enough test data for Macau, skipping!\n",
      "Madagascar shift is 2\n",
      "Malawi shift is 1\n",
      "Malaysia shift is 3\n",
      "Maldives shift is 0\n",
      "Not enough test data for Mali, skipping!\n",
      "Malta shift is 1\n",
      "Not enough test data for MarshallIslands, skipping!\n",
      "Mauritania is sparse: 338 132 48\n",
      "Not enough test data for Mauritius, skipping!\n",
      "Mexico shift is 1\n",
      "Not enough test data for Micronesia, skipping!\n",
      "Not enough test data for Moldova, skipping!\n",
      "Not enough test data for Monaco, skipping!\n",
      "Not enough test data for Mongolia, skipping!\n",
      "Not enough test data for Montenegro, skipping!\n",
      "Morocco shift is 0\n",
      "Mozambique shift is 0\n",
      "Myanmar shift is -2\n",
      "Namibia shift is 0\n",
      "Not enough test data for Nauru, skipping!\n",
      "Nepal shift is 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:233: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Netherlands is sparse: 338 149 22\n",
      "Not enough test data for NewGuinea, skipping!\n",
      "NewZealand shift is 1\n",
      "Not enough test data for Nicaragua, skipping!\n",
      "Not enough test data for Niger, skipping!\n",
      "Nigeria shift is 2\n",
      "Not enough test data for NorthKorea, skipping!\n",
      "NorthMacedonia shift is 1\n",
      "Norway shift is 1\n",
      "Not enough test data for Oman, skipping!\n",
      "Pakistan shift is -1\n",
      "Not enough test data for Palau, skipping!\n",
      "Panama shift is 0\n",
      "Paraguay shift is 0\n",
      "Peru shift is 2\n",
      "Philippines shift is 2\n",
      "Poland shift is 0\n",
      "Portugal shift is 1\n",
      "Qatar shift is 0\n",
      "Romania shift is 0\n",
      "Russia shift is 2\n",
      "Rwanda shift is 4\n",
      "Not enough test data for SaintKitts, skipping!\n",
      "Not enough test data for SaintLucia, skipping!\n",
      "Not enough test data for Samoa, skipping!\n",
      "Not enough test data for San Marino, skipping!\n",
      "Not enough test data for SaoTome, skipping!\n",
      "SaudiArabia shift is 1\n",
      "Senegal shift is 0\n",
      "Serbia shift is 0\n",
      "Not enough test data for Seychelles, skipping!\n",
      "Not enough test data for Sierra Leone, skipping!\n",
      "Singapore is sparse: 338 164 24\n",
      "Slovakia shift is 1\n",
      "Slovenia shift is 1\n",
      "Not enough test data for SolomonIslands, skipping!\n",
      "Not enough test data for Somalia, skipping!\n",
      "SouthAfrica shift is 0\n",
      "SouthKorea shift is 0\n",
      "SouthSudan is sparse: 338 130 58\n",
      "Spain is sparse: 338 161 23\n",
      "SriLanka shift is 0\n",
      "Not enough test data for Sudan, skipping!\n",
      "Not enough test data for Suriname, skipping!\n",
      "Sweden is sparse: 338 69 9\n",
      "Switzerland shift is -1\n",
      "Not enough test data for Syria, skipping!\n",
      "Taiwan shift is 4\n",
      "Not enough test data for Tajikistan, skipping!\n",
      "Not enough test data for Tanzania, skipping!\n",
      "Thailand shift is 3\n",
      "Togo shift is 0\n",
      "Not enough test data for Tonga, skipping!\n",
      "Trinidad shift is -2\n",
      "Tunisia shift is -2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:233: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkey shift is -1\n",
      "Not enough test data for Turkmenistan, skipping!\n",
      "Not enough test data for Tuvalu, skipping!\n",
      "UAE shift is 0\n",
      "Uganda shift is 1\n",
      "UK shift is 0\n",
      "Ukraine shift is 0\n",
      "Uruguay shift is 0\n",
      "USA shift is 0\n",
      "Not enough test data for Uzbekistan, skipping!\n",
      "Not enough test data for Vanuatu, skipping!\n",
      "Not enough test data for Vatican, skipping!\n",
      "Not enough test data for Venezuela, skipping!\n",
      "Vietnam is sparse: 338 166 36\n",
      "Not enough test data for Yemen, skipping!\n",
      "Zambia shift is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tylim\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:233: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zimbabwe shift is -2\n",
      "Importing data to VDF...\n"
     ]
    }
   ],
   "source": [
    "if smoothing == True:\n",
    "    for k,v in smparams.items():\n",
    "        exec(k + '=v')\n",
    "    smooth_data(skiplist)\n",
    "\n",
    "import_datasets(datalist, vdfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job done!\n"
     ]
    }
   ],
   "source": [
    "subprocess.run(f\"{vensimpath} \\\"./ImportData.cmd\\\"\", check=True)\n",
    "\n",
    "copy_data(datalist, vdfname)\n",
    "print(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv(\"TestData.csv\", index_col=1,header=0)\n",
    "testdf.drop(columns='Time', inplace=True)\n",
    "display(testdf)\n",
    "\n",
    "newdf = pd.DataFrame().reindex_like(testdf)\n",
    "display(newdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
